# ğŸ§ RAGdio â€“ Audio-First RAG with Local + Cloud AI

**RAGdio** is an audio-focused RAG (Retrieval-Augmented Generation) framework combining transcription, semantic search, and multiple LLM backends â€” both local and cloud-based.

> ğŸ’¡ Built with clean code, adapters, interfaces, and a minimal full-stack (FastAPI + Vite).


<p align="center">
  <img 
    src="./docs/media/rag_mem_short.gif" 
    alt="query RAG + memory" 
    width="80%" 
    style="border-radius: 8px;" 
  />
</p>


### ğŸ§ª Current Status

> **RAGdio is intended for local use, testing, and development purposes.**  
> It's not production-ready and not designed for multi-user environments (yet).

- â— Great for **experimenting** with audio-to-RAG pipelines locally.
- âš™ï¸ Designed with **modularity** in mind â€” adapters, interfaces, and clean architecture.
- ğŸš« Not optimized for production load, scaling, or secure multi-user handling.
- ğŸ“Œ Requires manual setup of models (e.g., in Ollama) after first run.

---

Chat with memory/history feature (available in API and Local models)

### âœ… Check transcription before add to RAG
<p align="center">
  <img 
    src="./docs/media/add_element_to_rag_p1_v3.gif" 
    alt="Check provided transcription" 
    width="80%" 
    style="border-radius: 8px;" 
  />
</p>

### ğŸ” Query
<p align="center">
  <img 
    src="./docs/media/add_element_to_rag_p2.gif" 
    alt="Query RAG" 
    width="80%" 
    style="border-radius: 8px;" 
  />
</p>

---

## ğŸš€ Quick Start (with Docker)

```bash
git clone https://github.com/Kotmin/RAGdio
cd RAGdio

# 1. Configure environment variables
copy from examples or manually set up

# 2. Start services
docker compose up --build
```


Once everything is up and running, you can access:

- ğŸ–¥ï¸ **Frontend**: [http://localhost:5173](http://localhost:5173)  
- ğŸ“¡ **Backend (API docs)**: [http://localhost:8000/docs](http://localhost:8000/docs)

### âš ï¸ Note (Ollama Users)
The default `docker-compose` setup includes an **Ollama server** _without any model pre-installed_.  
After running `docker compose up`, you must install a model manually.

You can install **Zephyr** or any other supported model.  
ğŸ‘‰ See [**Scenario 1: Ollama + Zephyr (lightweight local)**](#-scenario-1-ollama--zephyr-lightweight-local) for model installation instructions.


Then open:

- **Frontend**: http://localhost:5173
- **Backend** (API docs): http://localhost:8000/docs
- if using ollama: `install model` ()

---

## âš™ï¸ .env Example (`RAGdio-backend/.env`)

```env
ASR_MODEL=whisper                  # whisper, whisper-api
OPENAI_API_KEY=sk-...              # for OpenAI-based features

VECTOR_BACKEND=qdrant             # qdrant
QDRANT_HOST=qdrant                # if starded within docker 
EMBEDDING_BACKEND=huggingface     # huggingface, openai

LLM_PROVIDER_TYPE=ollama          # openai, ollama, deepseek, local
LLM_RAG_MODE=rag_fallback         # rag_fallback, rag_strict

LOCAL_OLLM_MODEL=zephyr
LOCAL_OLLM_API_URL=http://localhost:11434/api/generate

DEBUG=true
```

### LLM RAG MODES (if available)

- "rag_strict" â€“ LLM can only answer from documents

- "rag_fallback" â€“ If docs are not useful or missing, LLM falls back to its own knowledge


## âš™ï¸ .env Example (`rag-audio-frontend.env`)

```env
VITE_API_BASE_URL=http://localhost:8000/api
```

## VITE_API_BASE_URL=http://localhost:8000/api

## ğŸ“¦ Supported ASR + LLM Backends

| Component     | Options                                 |
| ------------- | --------------------------------------- |
| Transcription | `whisper`, `whisper-api`                |
| Embeddings    | `huggingface`, `openai`                 |
| Vector DB     | `qdrant` (with Docker)                  |
| LLM Backends  | `openai`, `ollama`, `deepseek`, `local` |

---

## ğŸ§  LLM RAG Modes

| Mode           | Description                                                                 |
| -------------- | --------------------------------------------------------------------------- |
| `rag_strict`   | LLM must only answer using provided documents                               |
| `rag_fallback` | LLM can fallback to its own knowledge if context is unclear or insufficient |

---

## ğŸ“¼ Audio Transcription

**Supported formats**:

```
mp3, mp4, mpeg, mpga, m4a, wav, webm
```

We recommend `whisper` with `"medium"` or higher model for best local accuracy.

---

## ğŸ§ª Test Scenarios

### âœ… Scenario 1: Ollama + Zephyr (lightweight local)
* Zephyr can be replaced with any llm

#### Requirements

- ~3.5 GB disk (Zephyr model)
- 7 GB RAM
- ~1 GB VRAM (optional)

#### Setup

```bash
docker exec -it ollama ollama pull zephyr
```

.env:

```env
LLM_PROVIDER_TYPE=ollama
LOCAL_OLLM_MODEL=zephyr
```

#### Flow

1. Upload 3 audio files (MP3/WAV/WEBM/etc.)
2. Transcribe + ingest to RAG
3. Ask a contextual question (e.g. "What did Krzysio say?")
4. Ask general question (e.g. "What's the capital of Poland?")
5. Ask "What was my previous question?" (test memory)

---

### âœ… Scenario 2: DeepSeek LLM (local, 7B)

#### Requirements

- ~13â€“15 GB disk
- 8â€“16 GB RAM
- GPU highly recommended

#### Setup

.env:

```env
LLM_PROVIDER_TYPE=deepseek
```

Model downloads automatically via `transformers`.

Same test flow as Scenario 1.

---

### âœ… Scenario 3: Fully Remote (OpenAI)

.env:

```env
ASR_MODEL=whisper-api
OPENAI_API_KEY=sk-...
VECTOR_BACKEND=qdrant
QDRANT_HOST=qdrant
EMBEDDING_BACKEND=openai
LLM_PROVIDER_TYPE=openai
LLM_RAG_MODE=rag_fallback
```

No local models needed. Ideal for low-resource devices.

---

## ğŸ§  Chat Memory & Context

- Stored in `chat_id` (browser localStorage)
- Up to 10-turn rolling memory (user + assistant)
- Injected as summary/context for compatible models
- Shared to LLM as part of prompt (unless using OpenAIâ€™s own chain)

Sample structure:

```json
{
  "chat_id": "uuid...",
  "turns": [ { "role": "user", "content": "..." }, ... ],
  "summary": "optional summary",
  "context": "optional injected context"
}
```

---

## ğŸ’¾ System Requirements

| Stack              | Disk Usage | RAM     | VRAM    |
| ------------------ | ---------- | ------- | ------- |
| `ollama + zephyr`  | ~3.5 GB    | ~3 GB   | ~1 GB   |
| `deepseek-7b-chat` | ~13â€“15 GB  | 8â€“16 GB | ~10 GB  |
| `OpenAI (API)`     | None       | ~500 MB | None    |
| `Whisper (medium)` | ~2 GB      | ~2â€“3 GB | ~1â€“2 GB |
| `Qdrant`           | ~300 MB    | ~300 MB | None    |

---

## ğŸ— Architecture

- ğŸ§  **Langchain** for RAG + chaining
- ğŸ—ƒ **Qdrant** as vector DB (via Docker)
- ğŸ§± Clean adapter-based architecture (LLM, ASR, Embeddings)
- ğŸ™ Whisper for audio transcription
- ğŸŒ FastAPI backend + Vite frontend

---

## ğŸ›  Development Reference

```bash
# Run full app
docker compose up --build

# Access frontend
http://localhost:5173

# Access backend (API docs)
http://localhost:8000/docs

# Pull Zephyr model (inside container)
docker exec -it ollama ollama pull zephyr
```

---

## ğŸ“ Project Structure (Simplified)

```
RAGdio/
â”œâ”€â”€ RAGdio-backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ adapters/         # LLM, audio, embedding adapters
â”‚   â”‚   â”œâ”€â”€ services/         # RAG pipeline, chat memory
â”‚   â”‚   â”œâ”€â”€ routers/          # FastAPI routes
â”‚   â”‚   â””â”€â”€ core/             # Config, logging
â”œâ”€â”€ rag-audio-frontend/       # React + Tailwind UI
â”œâ”€â”€ docker-compose.yml
```

---

## ğŸ“ƒ License

MIT License. Built to tinker, break, and rebuild â€” have fun with audio-first RAG!
